"""
Utilities to generate training data for the SV detecting NN.
cmdoret, 20190131
"""
import numpy as np
import pandas as pd
import pickle
import glob
import cooler
from Bio import SeqIO, Seq
import json
from typing import Optional, Tuple
import hic_scrambler.sv as hsv
import hic_scrambler.BAM_functions as bm
import hic_scrambler.GCpercent_functions as gcp
import hic_scrambler.complexity_function as cf


class GenomeMixer(object):
    """
    Handles genome edition through different types of structural variations.
    Note that the editions made in the genome are be the opposite of what their
    name says. The SV generated in the genome will be "mirrored" in the library.
    For example, deleting a region in the genome is akin to inserting a region
    in the library.

    Examples
    --------
        mix = GenomeMixer("genome.fasta", "config.json", "profile="Dmel")
        mix.generate_sv()
        mix.edit_genome("new_genome.fasta")

    Attributes
    ----------
    genome_path : str
        Path to the input genome to mix.
    config_path : str
        Path to the JSON config file to use for generating SVs.
    config : dict
        SV properties in a nested dictionary, loaded from a single profile in
        the config file.
    chromsizes : dict
        Contains the size of each chromosome in the format {chrom: len, ...}.
    sv : pandas.DataFrame
        Contains all structural variants, once generated by `generate_sv()`

    """

    def __init__(
        self, genome_path: str, config_path: str, config_profile: Optional[str] = None,
    ):
        self.config_path = config_path
        self.config = self.load_profile(profile=config_profile)
        self.genome_path = genome_path
        self.chromsizes = self.load_chromsizes(self.genome_path)
        self.sv = None

    def load_profile(self, profile: Optional[str] = None):
        """
        Load SV profile from a JSON config file. The top level JSON object is a
        profile. A config file can have multiple profiles, but only one will be
        loaded. Each profile contains the 'SV_freq' property, which gives the
        number of SV per bp, and the `SV_types` object. The SV_types object
        contains one object per SV type. Each SV contains multiple properties.

        Parameters
        ----------
        profile : str
            Name of the profile to load in the config file.

        Returns
        -------
        dict :
            A dictionary of SV types structured like
            {"sv_type":{"property": value, ...}, ...}
        """
        config = json.load(open(self.config_path, "r"))
        if not profile:
            if len(config) > 1:
                print(
                    "You must specify a profile name if multiple profiles "
                    "appear in the JSON config file"
                )
                raise ValueError
            else:
                profile = config.keys()[0]

        return config[profile]

    @staticmethod
    def load_chromsizes(path: str) -> dict:
        """
        Loads a fasta file and returns a chromsizes dict.

        Parameters
        ----------
        path : str
            Path to the FASTA file to load.

        Returns
        -------
        chromsizes : dict
            A dictionary of chromosomes sizes format {"chrom": size}


        """
        records = SeqIO.parse(path, format="fasta")
        return {rec.id: len(str(rec.seq)) for rec in records}

    def generate_sv(self) -> pd.DataFrame:
        """
        Generates random structural variations, based on the parameters loaded
        from the instance's config file.
        # NOTE: Currently only implemented for inversions and deletions.

        Returns
        -------
        pandas.DataFrame :
            A dataframe where each row is a SV. columns represent
            sv_type, chrom, start, end.
        """
        # Relative abundance of each event type (placeholder values)
        # rel_abun = {"INV": 8, "DEL": 400, "DUP": 60, "INS": 160, "CNV": 350}
        all_chroms_sv = []
        for chrom, size in self.chromsizes.items():
            n_sv = round(size * self.config["SV_freq"])
            chrom_sv = pd.DataFrame(np.empty((n_sv, 9)))
            chrom_sv.columns = [
                "sv_type",
                "chrom",
                "breakpoint1",
                "breakpoint2",
                "breakpoint3",
                "sgn_bp1",
                "sgn_bp2",
                "sgn_bp3",
                "size",
            ]
            sv_count = 0
            for sv_name, sv_char in self.config["SV_types"].items():
                # multiply proportion of SV type by total SV freq desired to
                # get number of events of this type.
                n_event = round(n_sv * sv_char["prop"])

                # Safeguard rounding errors
                if sv_count + n_event > n_sv:
                    n_event -= (n_event + sv_count) - n_sv

                print("Generating {0} {1}".format(n_event, sv_name))
                for _ in range(n_event):
                    # Start position is random and length is picked from a normal
                    # distribution centered around mean length.
                    breakpoint1 = np.random.randint(size)
                    breakpoint2 = breakpoint1 + abs(
                        np.random.normal(
                            loc=sv_char["mean_size"], scale=sv_char["sd_size"]
                        )
                    )

                    # Make sure the inversion does not go beyond chromosome.
                    breakpoint2 = min(size, breakpoint2)

                    breakpoint3 = np.random.randint(size)

                    while (breakpoint3 >= breakpoint1) and (breakpoint3 <= breakpoint2):

                        breakpoint3 = np.random.randint(size)

                    chrom_sv.iloc[sv_count, :] = (
                        sv_name,
                        chrom,
                        breakpoint1,
                        breakpoint2,
                        breakpoint3,
                        "+-",
                        "+-",
                        "+-",
                        breakpoint2 - breakpoint1,
                    )
                    sv_count += 1
            all_chroms_sv.append(chrom_sv)

        out_sv = pd.concat(all_chroms_sv, axis=0)
        out_sv.breakpoint1, out_sv.breakpoint2, out_sv.breakpoint3 = (
            out_sv.breakpoint1.astype(int),
            out_sv.breakpoint2.astype(int),
            out_sv.breakpoint3.astype(int),
        )
        # Randomize rows in SV table to mix the order of different SV types
        out_sv = out_sv.sample(frac=1).reset_index(drop=True)
        self.sv = out_sv

    def save_edited_genome(self, fasta_out: str):
        """
        Apply computed SVs to the sequence and store the edited sequence into
        the target file in fasta format.

        Coordinates in self.sv are updated as the genome is modified.

        Parameters
        ----------
        fasta_out : str
            Path where the edited genome will be written in fasta format.
        """
        print("----------------------------------")
        print("AT THE BEGINNING:")
        print(self.sv)

        with open(fasta_out, "w") as fa_out:
            for rec in SeqIO.parse(self.genome_path, format="fasta"):
                mutseq = Seq.MutableSeq(str(rec.seq))

                for row_num in range(self.sv.shape[0]):

                    row = self.sv.iloc[row_num, :]
                    sv_type = row.sv_type

                    chrom, breakpoint1, breakpoint2 = (
                        row.chrom,
                        int(row.breakpoint1),
                        int(row.breakpoint2),
                    )
                    # NOTE: Only implemented for inversions for now.

                    if sv_type == "INV":

                        start = min(breakpoint1, breakpoint2)
                        end = max(breakpoint1, breakpoint2)

                        # Reverse complement to generate inversion
                        if chrom == rec.id:
                            mutseq[start:end] = hsv.inversion(mutseq[start:end])

                            sgns_bp = [
                                self.sv.sgn_bp1[row_num],
                                self.sv.sgn_bp2[row_num],
                            ]

                            sgn_start = sgns_bp[[breakpoint1, breakpoint2].index(start)]
                            sgn_end = sgns_bp[[breakpoint1, breakpoint2].index(end)]

                            # Update coordinates of other SVs in the INV region
                            self.sv.breakpoint1 = hsv.update_coords_inv(
                                start, end, self.sv.breakpoint1
                            )
                            self.sv.breakpoint2 = hsv.update_coords_inv(
                                start, end, self.sv.breakpoint2
                            )

                            self.sv.breakpoint3 = hsv.update_coords_inv(
                                start, end, self.sv.breakpoint3
                            )

                            # Update sgns
                            self.sv.sgn_bp1 = hsv.update_sgn_inversion(
                                start,
                                end,
                                sgn_start,
                                sgn_end,
                                self.sv.breakpoint1,
                                self.sv.sgn_bp1,
                            )

                            self.sv.sgn_bp2 = hsv.update_sgn_inversion(
                                start,
                                end,
                                sgn_start,
                                sgn_end,
                                self.sv.breakpoint2,
                                self.sv.sgn_bp2,
                            )

                            self.sv.sgn_bp3 = hsv.update_sgn_inversion(
                                start,
                                end,
                                sgn_start,
                                sgn_end,
                                self.sv.breakpoint3,
                                self.sv.sgn_bp3,
                            )

                    elif sv_type == "DEL":

                        if chrom == rec.id:

                            mutseq = hsv.deletion(start, end, mutseq)
                            # Shift coordinates on the right of DEL region

                            self.sv.breakpoint1 = hsv.update_coords_del(
                                start, end, self.sv.breakpoint1
                            )

                            self.sv.breakpoint2 = hsv.update_coords_del(
                                start, end, self.sv.breakpoint2
                            )
                            self.sv.breakpoint3 = hsv.update_coords_del(
                                start, end, self.sv.breakpoint3
                            )

                    elif sv_type == "TRA":

                        breakpoint3 = row.breakpoint3

                        if chrom == rec.id:

                            start = breakpoint1
                            end = breakpoint2
                            start_paste = breakpoint3

                            mutseq = hsv.translocation(start, end, start_paste, mutseq)

                            # Update coordinates
                            self.sv.breakpoint1 = hsv.update_coords_tra(
                                start, end, start_paste, self.sv.breakpoint1
                            )

                            self.sv.breakpoint2 = hsv.update_coords_tra(
                                start, end, start_paste, self.sv.breakpoint2
                            )
                            self.sv.breakpoint3 = hsv.update_coords_tra(
                                start, end, start_paste, self.sv.breakpoint3
                            )

                            # Make sure start is always lower than end
                            # self.sv.start, self.sv.end = hsv.swap(
                            #    self.sv.start, self.sv.end
                            # )

                    else:
                        raise NotImplementedError("SV type not implemented yet.")
                    print("----------------------------")
                    print(row_num)
                    print(sv_type)
                    print(len(mutseq))
                    print(self.sv)
                    print("----------------------------")

                self.sv.breakpoint1 = self.sv.breakpoint1.astype(int)
                self.sv.breakpoint2 = self.sv.breakpoint2.astype(int)
                self.sv.breakpoint3 = self.sv.breakpoint3.astype(int)
                # Discard SV that have been erased by others
                self.sv = self.sv.loc[
                    (abs(self.sv.breakpoint1 - self.sv.breakpoint2) > 1)
                    | (self.sv.sv_type == "DEL"),
                    :,
                ]
                self.sv.index = pd.RangeIndex(start=0, stop=len(self.sv.index), step=1)
                rec = SeqIO.SeqRecord(seq=mutseq, id=rec.id, description="")

                SeqIO.write(rec, fa_out, format="fasta")


def save_sv(sv_df: pd.DataFrame, clr: cooler.Cooler, path: str):
    """
    Saves the structural variant (SV) table into a text file.
    The order of SVs in that table matches the order in which they were
    introduced in the genome.
    """
    full_sv = sv_df.copy()

    full_sv.to_csv(path, sep="\t", index=False)


def pos_to_coord(
    clr: cooler.Cooler, sv_df: pd.DataFrame
) -> Tuple[pd.DataFrame, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    Converts start - end genomic positions from structural variations to breakpoints
    in matrix coordinates.

    Parameters
    ----------
    clr : cooler.Cooler
        The cooler object containing Hi-C data
    sv_df : pandas.DataFrame
        A dataframe containg the type and genomic start-end coordinates of
        strucural variations as given by generate_sv().

    Returns
    -------
    sv_df: pd.DataFrame
        DataFrame with rearranged columns.

    breakpoints : numpy.ndarray of int
        A N x 2 numpy array of numeric values representing X, Y coordinates of structural
        variations breakpoints in the matrix.
    labels : numpy.ndarray of str
        An N X 1 array of labels corresponding to SV type.
    pos_BP : numpy.ndarray of int
        A N x 2 numpy array of numeric values representing position of breakpoints in the BP.
    Chrom : numpy.ndarray of str
        A N x 1 numpy array of chrom for each position.    
    index_TRA: numpy.ndarray of int
        A N x 1 numpy array. If posBP[i] correspond to a translocation,  index_tra[i] = 1.
        Otherwise, index_tra[i] = 0.
    """
    # Get coordinates to match binning
    res = clr.binsize

    # Assign matrix coordinate (fragment index) to each breakpoint
    sv_df["coord_bp1"] = sv_df.breakpoint1 // res
    sv_df["coord_bp2"] = sv_df.breakpoint2 // res
    sv_df["coord_bp3"] = sv_df.breakpoint3 // res

    breakpoints = np.vstack([sv_df.coord_bp1.values, sv_df.coord_bp2.values]).T
    breakpoints.astype(int)
    labels = np.array(sv_df.sv_type.tolist())

    posBP1 = sv_df.breakpoint1.values
    posBP2 = sv_df.breakpoint2.values
    posBP3 = sv_df[sv_df["sv_type"] == "TRA"].breakpoint3.values
    pos_BP = np.array([posBP1, posBP2]).T

    index_TRA = np.concatenate(
        (
            (sv_df["sv_type"] == "TRA").values * 1,
            (sv_df["sv_type"] == "TRA").values * 1,
            np.ones(len(posBP3)),
        ),
    )

    chroms = sv_df.chrom.values

    cols = [
        "sv_type",
        "chrom",
        "breakpoint1",
        "breakpoint2",
        "breakpoint3",
        "coord_bp1",
        "coord_bp2",
        "coord_bp3",
        "sgn_bp1",
        "sgn_bp2",
        "sgn_bp3",
        "size",
    ]
    sv_df = sv_df[cols]

    return sv_df, breakpoints, labels, pos_BP, chroms, index_TRA


def subset_mat(
    clr: cooler.Cooler,
    coords: np.ndarray,
    coordsBP: np.ndarray,
    labels: np.ndarray,
    chroms: np.ndarray,
    win_size: int,
    binsize: int,
    rundir: str,
    tmpdir: str,
    prop_negative: float = 0.5,
    pixel_tolerance: int = 3,
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    Samples evenly sized windows from a matrix. Windows are centered around
    input coordinates. Windows and their associated labels are returned. A number
    of random negative windows (without sv) will be added to the output so that
    there are prop_negative negative negative windows in the output.

    Parameters
    ----------
    clr : cooler.Cooler
        Cooler object containing Hi-C data.
    coords : numpy.ndarray of ints
        Pairs of coordinates for which subsets should be generated. A window
        centered around each of these coordinates will be sampled. Dimensions
        are [N, 2].
    labels : numpy.ndarray of ints
        labels of for SVs
    win_size : int
        Size of windows to sample from the matrix.
    binsize: int
        The resolution of the matrices we generate, in basepair.
    rundir : str
        Name of the directory where genome.fa is.
    tmpdir : str
        Name of the temporary directory.
    prop_negative : float
        The proportion of windows without SVs desired. If set to 0.5, when given
        a list of 23 SV, the function will output 46 observations (windows); 23
        without SV (picked randomly in the matrix) and 23 with SV.
    pixel_tolerance: int
        Pixel of tolerance when we detect an SV.
    chrom_name: str 
        Name of the chromosome.
    Returns
    -------
    x : numpy.ndarray of floats
        The 3D feature vector to use as input in a keras model to detect zones of SV.
        Dimensions are [N, win_size, win_size].
    y : numpy.ndarray of ints
        The 1D label vector of N values to use as prediction in a keras model to detect zones of SV.
    percents : numpy.ndarray of floats
        An array with the evolution of GC% for each coord.
    start_reads : numpy.ndarray of int
        An array with the smoothed number of reads which start at each position.  
    end_reads : numpy.ndarray of int
        An array with the smoothed number of reads which end at each position. 
    nreads : numpy.ndarray of int
        An array with the smoothed number of reads at each position.     
    coords_windows : numpy.ndarray of int
        Coordinates of the window we use for our compute. 
    complexity : numpy.ndarray of int
        Complexity of sequence near each position. 
    """
    h, w = clr.shape
    i_w = int(h - win_size // 2)
    j_w = int(w - win_size // 2)
    sv_to_int = {"INV": 1, "DEL": 2, "TRA": 3}
    size_train = 100
    size_win_bp = 2
    # Only keep coords far enough from borders of the matrix

    valid_coords = np.where(
        (coords[:, 0] > int(win_size / 2))
        & (coords[:, 1] > int(win_size / 2))
        & (coords[:, 0] < i_w)
        & (coords[:, 1] < j_w)
    )[0]

    coords = coords[valid_coords, :]

    coords = coords.reshape(
        (-1, 1)
    )  # Allow to have an image at the beginning and at the end of the sv

    labels = labels[valid_coords]
    coordsBP = coordsBP[valid_coords, :]
    chroms = chroms[valid_coords]

    np.save(rundir + "/coordsBP.npy", coordsBP)

    # Number of windows to generate (including negative windows)
    n_windows = int(coords.shape[0] // (1 - prop_negative))
    x = np.zeros((n_windows, win_size, win_size), dtype=np.float64)
    y = np.zeros(n_windows, dtype=np.int64)
    coords_windows = np.zeros((n_windows, size_train + 1), dtype=np.int64)
    percents_GC = np.zeros((n_windows, size_train + 1), dtype=np.float64)
    start_reads = np.zeros((n_windows, size_train + 1), dtype=np.int64)
    end_reads = np.zeros((n_windows, size_train + 1), dtype=np.int64)
    n_reads = np.zeros((n_windows, size_train + 1), dtype=np.int64)
    complexity = np.zeros((n_windows, size_train + 1), dtype=np.int64)

    if win_size >= min(h, w):
        print("Window size must be smaller than the Hi-C matrix.")
    halfw = win_size // 2
    # Getting SV windows
    coords = coords.astype(int)
    for i in range(coords.shape[0]):
        print(i)
        print(
            coordsBP[i, 0]
        )  # We will create the features only for the first coord (no reason to do that).
        c = coords[i]
        try:
            win = clr.matrix(sparse=False, balance=False)[
                (c[0] - halfw) : (c[0] + halfw), (c[0] - halfw) : (c[0] + halfw),
            ]
        except TypeError:
            breakpoint()
        x[i, :, :] = win
        y[i] = sv_to_int[labels[i]]

        c_beg = coordsBP[i, 0] - size_train // 2
        c_end = coordsBP[i, 0] + size_train // 2

        for c_ in range(c_beg, c_end):

            coords_windows[i, c_ - c_beg] = c_
            seq = gcp.load_seq(
                rundir + "/mod_genome.fa",
                chroms[i],
                c_ - size_win_bp // 2,
                c_ + size_win_bp // 2 + 1,
            )
            percents_GC[i, c_ - c_beg] = gcp.percent_GC(seq)
            complexity[i, c_ - c_beg] = cf.lempel_complexity(seq)

        region = (
            chroms[i]
            + ":"
            + str(c_beg - size_win_bp // 2)
            + "-"
            + str(c_end + size_win_bp // 2 + 1)
        )

        start_read, end_read = bm.bam_region_read_ends(
            file=tmpdir + "/scrambled.for.bam", region=region, side="both"
        )
        n_read = bm.bam_region_coverage(
            file=tmpdir + "/scrambled.for.bam", region=region
        )

        start_reads[i] = (
            start_read
            + np.concatenate((start_read[1:], np.zeros(1)))
            + np.concatenate((np.zeros(1), start_read[: len(start_read) - 1]))
        )[1:-1] // 3
        end_reads[i] = (
            end_read
            + np.concatenate((end_read[1:], np.zeros(1)))
            + np.concatenate((np.zeros(1), end_read[: len(end_read) - 1]))
        )[1:-1] // 3
        n_reads[i] = (
            n_read
            + np.concatenate((n_read[1:], np.zeros(1)))
            + np.concatenate((np.zeros(1), n_read[: len(n_read) - 1]))
        )[1:-1] // 3

    # Getting negative windows
    neg_coords = set()

    for i in range(coords.shape[0], n_windows):
        print(i)
        tries = 0
        c = np.random.randint(win_size // 2, i_w)
        # this coordinate must not exist already
        while (c in coords[:, 0]) or (c in neg_coords):
            print("{} is already used. Trying another position...".format(c))
            # If unable to find new coords, just return output until here
            if tries > 100:
                return x[:i, :, :], y[:i]
            neg_coords.add(c)
            c = np.random.randint(win_size // 2, i_w)
            tries += 1
        win = clr.matrix(sparse=False, balance=False)[
            (c - halfw) : (c + halfw), (c - halfw) : (c + halfw)
        ]
        x[i, :, :] = win
        y[i] = 0
        x = x.astype(int)

        c_beg = c * binsize - size_train // 2
        c_end = c * binsize + size_train // 2

        ind_chroms = np.random.randint(len(chroms))
        for c_ in range(c_beg, c_end):

            coords_windows[i, c_ - c_beg] = c_
            seq = gcp.load_seq(
                rundir + "/mod_genome.fa",
                chroms[ind_chroms],
                c_ - size_win_bp // 2,
                c_ + size_win_bp // 2 + 1,
            )
            percents_GC[i, c_ - c_beg] = gcp.percent_GC(seq)
            complexity[i, c_ - c_beg] = cf.lempel_complexity(seq)

        region = (
            chroms[ind_chroms]
            + ":"
            + str(c_beg - size_win_bp // 2)
            + "-"
            + str(c_end + size_win_bp // 2 + 1)
        )

        start_read, end_read = bm.bam_region_read_ends(
            file=tmpdir + "/scrambled.for.bam", region=region, side="both"
        )
        n_read = bm.bam_region_coverage(
            file=tmpdir + "/scrambled.for.bam", region=region
        )

        start_reads[i] = (
            start_read
            + np.concatenate((start_read[1:], np.zeros(1)))
            + np.concatenate((np.zeros(1), start_read[: len(start_read) - 1]))
        )[1:-1] // 3
        end_reads[i] = (
            end_read
            + np.concatenate((end_read[1:], np.zeros(1)))
            + np.concatenate((np.zeros(1), end_read[: len(end_read) - 1]))
        )[1:-1] // 3
        n_reads[i] = (
            n_read
            + np.concatenate((n_read[1:], np.zeros(1)))
            + np.concatenate((np.zeros(1), n_read[: len(n_read) - 1]))
        )[1:-1] // 3

    return (
        x,
        y,
        percents_GC,
        start_reads,
        end_reads,
        n_reads,
        coords_windows,
        complexity,
    )


def slice_genome(path: str, out_path: str, slice_size: int = 1000) -> str:
    """
    Given an input fasta file, slice a random region of a random chromosome and
    save it into a new fasta file.

    Parameters
    ----------
    path : str
        Path to the input fasta file.
    out_path: str
        Path to the output sliced fasta file.
    slice_size : int
        Size of the region to extract, in basepairs.

    Returns
    -------
    ucsc : str
        UCSC format string representing the region that was sliced.
    """

    # Generate a mapping of all chromosome names and their sizes
    chrom_sizes = GenomeMixer.load_chromsizes(path)

    # Exclude chromosomes smaller than slice_size
    rm_chroms = [ch for ch, size in chrom_sizes.items() if size < slice_size]
    for chrom in rm_chroms:
        del chrom_sizes[chrom]

    # Get list of valid chromosomes
    chrom_names = list(chrom_sizes.keys())

    # Pick a random region of slice_size bp in a random chromosome and write it
    picked_chrom = np.random.choice(chrom_names, size=1)[0]
    start_slice = int(
        np.random.randint(low=0, high=chrom_sizes[picked_chrom] - slice_size, size=1)
    )
    end_slice = int(start_slice + slice_size)
    with open(out_path, "w") as sub_handle:
        for rec in SeqIO.parse(path, "fasta"):
            if rec.id == picked_chrom:
                rec.seq = rec.seq[start_slice:end_slice]
                SeqIO.write(rec, sub_handle, "fasta")
                break

    # Report the selected region in UCSC format
    ucsc = f"{picked_chrom}:{start_slice}-{end_slice}"
    return ucsc
